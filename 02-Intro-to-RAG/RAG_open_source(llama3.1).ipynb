{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1K5u1xyfHBG0NmB2OwLz0U-bQqwnwQkB3\n",
    "\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to RAG Applications\n",
    "\n",
    "This notebook creates a simple RAG (Retrieval-Augmented Generation) system to answer questions from a PDF document using an open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"FAQ_GDG.pdf\"\n",
    "\n",
    "# We'll be using Llama 3.1 8B for this example.\n",
    "MODEL = \"llama3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF document\n",
    "\n",
    "starting by loading the PDF document and breaking it down into separate pages.\n",
    "\n",
    "<img src='images/documents.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pypdf in c:\\users\\msi\\miniconda3\\envs\\gn\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from pypdf) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 12\n",
      "Length of a page: 2037\n",
      "Content of a page: W h y w a s m y a c c o u n t t u r n e d t o p r i v a t e ? \n",
      " If we r easonably belie v e content in y our pr o ﬁ le violates our content policy , y our account will be switched t o priv ate and the content in y our pr o ﬁ le will be deleted. Y ou won 't be able t o mak e y our account public again for at least 60 da ys. Google also r eser v es the right t o suspend or terminate y our access t o the ser vices or delete y our Google Account, as described in the T aking action in case of pr oblems section of the Google T erms of Ser vice. \n",
      "W h a t h a p p e n s w h e n I i n t e g r a t e m y p r o ﬁ l e w i t h a t h i r d - p a r t y a p p o r s e r v i c e ? \n",
      " If y ou authoriz e an application t o access y our Google De v eloper Pr ogr am pr o ﬁ le, that application will be able t o see y our pr o ﬁ le information, e v en if y ou ha v e not made y our pr o ﬁ le public. Learn mor e about how t o manage thir d-par ty apps and ser vices with access t o y our account. \n",
      "C a n I t r a n s f e r o r m e r g e m y a c c o u n t w i t h a n o t h e r p r o ﬁ l e ? \n",
      " Y ou cannot tr ansf er or mer ge y our pr o ﬁ le with another account. It' s encour aged t o use y our personal account (when appr opriate) for y our Google De v eloper Pr ogr am pr o ﬁ le t o ensur e y ou r etain all the badges and information. \n",
      "W h y c a n ' t I c r e a t e a p r o ﬁ l e w i t h m y G o o g l e W o r k s p a c e a c c o u n t ? \n",
      " The Google De v eloper Pr ogr am pr o ﬁ le suppor ts Google W orkspace account types; howe v er , if y ou ar e getting an err or y ou might need y our or ganization ' s administr at or t o enable access t o the Google De v elopers ser vice. \n",
      " F or mor e information, see T urn Google De v elopers on or off for users . \n",
      "W h e r e s h o u l d I ﬁ l e i s s u e s o r f e e d b a c k ? \n",
      " If y ou encounter any issues or want t o pr o vide f eedback on anything r elated t o y our Google De v eloper Pr ogr am pr o ﬁ le, click Send F eedback at the bott om of y our Google De v eloper Pr ogr am pr o ﬁ le page . \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages in chunks\n",
    "\n",
    "Pages are too long, so let's split pages into different chunks.\n",
    "\n",
    "<img src='images/splitter.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 36\n",
      "Length of a chunk: 582\n",
      "Content of a chunk: H o w d o I e d i t m y p r o ﬁ l e ? \n",
      " Y ou can edit y our Google De v eloper Pr ogr am pr o ﬁ le b y going t o de v elopers.google.com/pr o ﬁ le/u/me . \n",
      "W h a t h a p p e n s i f I m a k e m y p r o ﬁ l e p u b l i c ? \n",
      " Making y our pr o ﬁ le public mak es it viewable b y any one online. This includes y our name, image, r ole, company or school, bio, badges y ou'v e r eceiv ed, stats, and y our social media links (including GitHub, GitLab, X, Link edIn, and Stack Ov er ﬂ ow). Y our pages sa v ed, pages r ated, and e v ents attended ar e not par t of y our public pr o ﬁ le.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the chunks in a vector store\n",
    "\n",
    "We can now generate embeddings for every chunk and store them in a vector store.\n",
    "we will use FAISS: Facebook AI Similarity Search\n",
    "\n",
    "<img src='images/vectorstore.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-huggingface\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\miniconda3\\envs\\gn\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\MSI\\miniconda3\\envs\\gn\\lib\\site-packages\\beartype\\_util\\error\\utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.\n",
      "  warn(message, cls)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#ST = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "ST = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "query= \"hello \"\n",
    "embedded_query = ST.encode(query)\n",
    "embedded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings # new lib\n",
    "\n",
    "embed_model = \"nomic-embed-text\" # we can use same embed model as llama3.1 as they came in pairs\n",
    "embeddings = OllamaEmbeddings(model=embed_model)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a retriever\n",
    "\n",
    "We can use a retriever to find chunks in the vector store that are similar to a supplied question.\n",
    "\n",
    "<img src='images/retriever1.png' width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f09d1b5a-f8c0-40ec-b9b3-37e20d6d7a52', metadata={'source': 'FAQ_GDG.pdf', 'page': 9}, page_content='What doesaGDSCleaddo?\\nIn gener al, GDSC leaders ar e focused on the following ar eas: \\n ● Star t a club - W ork with y our univ ersity or college t o star t a student club. Select a cor e team and faculty advisor t o suppor t. ● Host workshops - Gr ow student knowledge on de v eloper pr oducts and platforms thr ough hands-on workshops and e v ents.'),\n",
       " Document(id='b6891c48-3f1c-4688-b512-2109cb67e6be', metadata={'source': 'FAQ_GDG.pdf', 'page': 11}, page_content='What isthetimecommitment?\\nGDSC Leads should be a v ailable t o run an e v ent ideally once a month, and at least e v er y thr ee months t o r emain an activ e GDSC chapter . Additionally , running a GDSC is a one y ear commitment. \\nTimeline\\nWhat isthetimelineforapplyingfortheGDSCLeadposition?\\nW e accept applications once per y ear , between April and A ugust. Please follow this page for the new deadlines and the GDSC e v ents platform t o check curr ent chapters.'),\n",
       " Document(id='118e0bd1-26d0-45eb-807a-ed653380740c', metadata={'source': 'FAQ_GDG.pdf', 'page': 9}, page_content='GDSCFAQ\\nThepurposeof thisdocument istocapturefrequentlyaskedquestionsabout theGDSCprogram.\\nJoinGDSC\\nWhoshouldjoinGoogleDeveloperStudent Clubs?\\nCollege and univ ersity students ar e encour aged t o join Google De v eloper Student Clubs. \\n CanI joinmultiplechapters?\\nY ou can par ticipate in e v ents or ganiz ed b y multiple chapters, howe v er if y ou decide t o dedicate y ourself t o become a GDSC Lead or Cor e T eam Member , y ou will be o ﬃ cially assigned t o one chapter . \\n What doesaGDSCleaddo?\\nIn gener al, GDSC leaders ar e focused on the following ar eas:'),\n",
       " Document(id='54de21a4-135f-4d1c-b3a2-8c3040e31c0d', metadata={'source': 'FAQ_GDG.pdf', 'page': 10}, page_content=\"Ther e ar e a f ew steps y ou need t o tak e t o apply t o be an or ganiz er . \\n ● See if ther e ’ s a curr ent Google De v eloper Student Club on y our campus or near y ou. W e encour age y ou t o par ticipate or e v en help or ganiz e an e v ent t o gain pr actical experience! ● Read the community guidelines . ● Submit y our GDSC Lead application . ● W e 'll r e view y our submission and get back t o y ou via email as soon as possible. \\nBenefits\\nWhat arethebenefitsof becomingaGDSClead?\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"what is GDSC ?\")\n",
    "# 4 k- similar chunks by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n",
    "\n",
    "We'll be using Ollama to load the local model in memory. After creating the model, we can invoke it with a question to get the response back.\n",
    "\n",
    "<img src='images/model1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'docstore', 'document_loaders', 'utils', 'vectorstores']\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "print(dir(langchain_community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last update in April 2023, Joe Biden is the President of the United States. He took office on January 20, 2021, succeeding Donald Trump as the 46th President of the United States. Please note that this information might change over time due to elections or other political developments.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-02-19T07:17:57.2439939Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 55750485200, 'load_duration': 21823367900, 'prompt_eval_count': 19, 'prompt_eval_duration': 5299367000, 'eval_count': 65, 'eval_duration': 28600090000}, id='run-9860c6a9-dac5-4051-94f8-f43eef922757-0', usage_metadata={'input_tokens': 19, 'output_tokens': 65, 'total_tokens': 84})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=MODEL, temperature=0)\n",
    "model.invoke(\"Who is the president of the United States?\") # eww whats is that \"AIMEssage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(\n",
    "    temperature=0,\n",
    "    model= \"llama-3.3-70b-versatile\",\n",
    "    api_key=\"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\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the model's response\n",
    "\n",
    "The response from the model is an `AIMessage` instance containing the answer. We can extract the text answer by using the appropriate output parser. We can connect the model and the parser using a chain.\n",
    "\n",
    "<img src='images/parser1.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunisia is a country located in the Maghreb region of North Africa. It is situated in the northern part of the African continent, bordering the Mediterranean Sea.\n",
      "\n",
      "Here are the geographical details of Tunisia's location:\n",
      "\n",
      "* Continent: Africa\n",
      "* Region: Maghreb (North Africa)\n",
      "* Borders:\n",
      "\t+ North: Mediterranean Sea\n",
      "\t+ East: Libya\n",
      "\t+ South: Algeria\n",
      "\t+ West: Algeria\n",
      "* Coordinates: 34°N latitude, 9°E longitude\n",
      "* Capital city: Tunis (located in the northeastern part of the country)\n",
      "\n",
      "Tunisia is a relatively small country, with a total area of approximately 163,610 square kilometers (63,170 square miles). It has a diverse geography, featuring a mix of coastal plains, mountains, and deserts. The country's strategic location at the crossroads of Europe, Africa, and the Middle East has made it an important hub for trade and cultural exchange throughout history.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "print(chain.invoke(\"where tunisia is located\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also want to provide the model with the context from the PDF file. We can use a prompt template to define and reuse the prompt we'll use with the model.\n",
    "\n",
    "\n",
    "<img src='images/prompt1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the prompt to the chain\n",
    "\n",
    "We can now chain the prompt with the model and the parser.\n",
    "\n",
    "<img src='images/chain11.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the retriever to the chain\n",
    "\n",
    "Finally, we can connect the retriever to the chain to get the context from the vector store.\n",
    "\n",
    "context is list of 4 most similar\n",
    "\n",
    "<img src='images/chain22.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the chain to answer questions\n",
    "\n",
    "Finally, we can use the chain to ask questions that will be answered using the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is GDG ?\n",
      "Answer: I don't know\n",
      "*************************\n",
      "\n",
      "Question: What is GDSC ?\n",
      "Answer: I don't know\n",
      "*************************\n",
      "\n",
      "Question: What is GDE?\n",
      "Answer: I don't know\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is GDG ?\",\n",
    "    \"What is GDSC ?\",\n",
    "    \"What is GDE?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"how many members in GDG carthage ? \"\n",
    "\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be a GDE, you need to: \\n1. Have solid expertise in an area featuring Google technology.\\n2. Display significant contributions in the developer community.\\n3. Be able to articulate clearly and provide meaningful advice.\\n4. Be 18+ years old.\\n5. Be able to communicate in English.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"how can i be a GDE \"\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, there is no cost to join a chapter or attend workshops.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"do i need to pay any fees to be a member in GDG and attend workshops ? \"\n",
    "\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do next ? \n",
    "\n",
    "Play with the size of chunks\n",
    "\n",
    "Try different documents \n",
    "\n",
    "Try multiple documents not just one.\n",
    "\n",
    "Different model other than llama3.1 \n",
    "\n",
    "Try different embedding models.\n",
    "\n",
    "Try different vectorstore databases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gn",
   "language": "python",
   "name": "gn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
